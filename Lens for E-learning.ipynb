{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3deff9fb",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3b7a5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2 \n",
    "import pytesseract as pt\n",
    "from PIL import Image\n",
    "pt.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract\"\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58220dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\ROSHINI R\n",
      "[nltk_data]     S\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\ROSHINI R\n",
      "[nltk_data]     S\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ROSHINI R S\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm',disable=['ner','textcat'])\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "017a5f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83459af6",
   "metadata": {},
   "source": [
    "### OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9443bde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OCR\n",
    "def img_txt(img_path):\n",
    "    global text\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img, None, fx = 1,fy = 1)\n",
    "    kernel = np.ones((1, 1), np.uint8)\n",
    "    img = cv2.dilate(img, kernel, iterations=1)\n",
    "    img = cv2.erode(img, kernel, iterations=1)\n",
    "    blurred=cv2.GaussianBlur(img,(5,5),0)\n",
    "    edged=cv2.Canny(blurred,30,50)\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    adpt = cv2.adaptiveThreshold(gray,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY, 85,10)\n",
    "    text = pt.image_to_string(adpt)\n",
    "    text = text.replace('\\n',' ')\n",
    "    print(text)\n",
    "    cv2.imshow(\"adaptive threshold\",adpt)\n",
    "    cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c083f1",
   "metadata": {},
   "source": [
    "### Getting Input from the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f79e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inupt image through webcam\n",
    "cam = cv2.VideoCapture(0)\n",
    "\n",
    "cv2.namedWindow(\"Capture\")\n",
    "\n",
    "img_counter = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "    if not ret:\n",
    "        print(\"failed to grab frame\")\n",
    "        break\n",
    "    \n",
    "    cv2.imshow(\"Capture\", frame)\n",
    " \n",
    "    k = cv2.waitKey(1)\n",
    "    if k%256 == 27:\n",
    "        # ESC pressed\n",
    "        print(\"Escape hit, closing...\")\n",
    "        break\n",
    "    elif k%256 == 32:\n",
    "        # SPACE pressed  \n",
    "        img_name = \"opencv_frame_{}.png\".format(img_counter)\n",
    "        cv2.imwrite(img_name, frame)\n",
    "        print(\"{} written!\".format(img_name))\n",
    "        img_counter += 1\n",
    "    \n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0659d805",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the image from local file\n",
    "img_name = 'inputimg.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d905ba07",
   "metadata": {},
   "source": [
    "### Display raw text from the Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb4ba1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanned text:\n",
      "\n",
      "   Artificial intelligence (Al) makes it possible for machines to learn from experience, adjust to new inputs and perform human-like tasks. Most Al examples that you hear about today - from chess-playing computers to self-driving cars - rely heavily on deep learning and natural language processing. Using these technologies, computers can be trained to accomplish specific tasks by processing large amounts of data and recognizing patterns in the data. \f",
      "\n"
     ]
    }
   ],
   "source": [
    "#Process the image and print text\n",
    "print(\"Scanned text:\\n\")\n",
    "img_txt(img_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e98c6d",
   "metadata": {},
   "source": [
    "### Summarize the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f939463f",
   "metadata": {},
   "source": [
    "### Using Spacy to summarise the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d68190bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Al', 2), ('tasks', 2), ('computers', 2), ('processing', 2), ('data', 2)]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en')\n",
    "doc = nlp(text)\n",
    "type(doc)\n",
    "len(list(doc.sents))\n",
    "keyword = []\n",
    "stopwords = list(STOP_WORDS)\n",
    "pos_tag = ['PROPN', 'ADJ', 'NOUN', 'VERB']\n",
    "for token in doc:\n",
    "    if(token.text in stopwords or token.text in punctuation):\n",
    "        continue\n",
    "    if(token.pos_ in pos_tag):\n",
    "        keyword.append(token.text)\n",
    "        \n",
    "freq_word = Counter(keyword)\n",
    "print(freq_word.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7adb59d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Al', 1.0),\n",
       " ('tasks', 1.0),\n",
       " ('computers', 1.0),\n",
       " ('processing', 1.0),\n",
       " ('data', 1.0)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_freq = Counter(keyword).most_common(1)[0][1]\n",
    "for word in freq_word.keys():  \n",
    "        freq_word[word] = (freq_word[word]/max_freq)\n",
    "freq_word.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6162dd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_strength={}\n",
    "for sent in doc.sents:\n",
    "    for word in sent:\n",
    "        if word.text in freq_word.keys():\n",
    "            if sent in sent_strength.keys():\n",
    "                sent_strength[sent]+=freq_word[word.text]\n",
    "            else:\n",
    "                sent_strength[sent]=freq_word[word.text]\n",
    "# print(sent_strength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b95d602",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarized_sentences = nlargest(4, sent_strength, key=sent_strength.get)\n",
    "# print(summarized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c3ca6f2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized text: \n",
      "\n",
      "Most Al examples that you hear about today - from chess-playing computers to self-driving cars - rely heavily on deep learning and natural language processing. Using these technologies, computers can be trained to accomplish specific tasks by processing large amounts of data and recognizing patterns in the data. \f",
      "    Artificial intelligence (Al) makes it possible for machines to learn from experience, adjust to new inputs and perform human-like tasks.\n"
     ]
    }
   ],
   "source": [
    "final_sentences = [ w.text for w in summarized_sentences ]\n",
    "summary = ' '.join(final_sentences)\n",
    "print(\"Summarized text: \\n\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0395fac",
   "metadata": {},
   "source": [
    "### Extracting key phrases from the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08273d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords: \n",
      " [('chess-playing computers', 8.0), ('accomplish specific tasks', 8.0), ('self-driving cars', 9.0), ('natural language processing', 9.0), ('processing large amounts', 9.0)]\n"
     ]
    }
   ],
   "source": [
    "import RAKE\n",
    "import operator\n",
    "stop_dir = r\"C:\\Users\\ROSHINI R S\\anaconda3\\Lib\\site-packages\\RAKE\\stoplists\\SmartStopList.txt\"\n",
    "rake_object=RAKE.Rake(stop_dir)\n",
    "def Sort_Tuple(tup):\n",
    "    tup.sort(key=lambda x: x[1])\n",
    "    return tup\n",
    "keywords = Sort_Tuple(rake_object.run(summary))[-5:]\n",
    "print(\"Keywords: \\n\",keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "376fb671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyphrases: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['chess-playing computers',\n",
       " 'accomplish specific tasks',\n",
       " 'self-driving cars',\n",
       " 'natural language processing',\n",
       " 'processing large amounts']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Phrase=[]\n",
    "for i in keywords:\n",
    "    Phrase.append(i[0])\n",
    "print(\"Keyphrases: \")\n",
    "Phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffeb50d6",
   "metadata": {},
   "source": [
    "### Finding relevant links from the keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311ebc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Automate google search for getting relevant links\n",
    "import requests\n",
    "import urllib.request\n",
    "import re\n",
    "\n",
    "try:\n",
    "    from googlesearch import search\n",
    "except ImportError:\n",
    "    print(\"No module named 'google' found\")\n",
    "Website_link=[]\n",
    "website_video_link=[]\n",
    "youtube_link=[]\n",
    "news_link=[]\n",
    "\n",
    "for i in Phrase:\n",
    "    for j in search(i, tld=\"co.in\", num=1, start=0,stop=1,pause=2,safe='on'):\n",
    "        Website_link.append(j)\n",
    "    for k in search(i, tld=\"co.in\", num=1, start=0,stop=1,pause=2,tpe=\"vid\",safe='on'):\n",
    "        website_video_link.append(k)\n",
    "    for l in search(i, tld=\"co.in\", num=1, start=0,stop=1,pause=2,tpe=\"nws\",safe='on'):\n",
    "        news_link.append(l)\n",
    "\n",
    "for m in Phrase:\n",
    "  query=m.replace(\" \",\"+\")\n",
    "  html=urllib.request.urlopen(\"https://www.youtube.com/results?search_query=\"+query)\n",
    "  video_ids = re.findall(r\"watch\\?v=(\\S{11})\", html.read().decode())\n",
    "  link=(\"https://www.youtube.com/watch?v=\" + video_ids[0])\n",
    "  youtube_link.append(link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc7d1bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Key phrase  \\\n",
      "0      chess-playing computers   \n",
      "1    accomplish specific tasks   \n",
      "2            self-driving cars   \n",
      "3  natural language processing   \n",
      "4     processing large amounts   \n",
      "\n",
      "                                        Website Link  \\\n",
      "0       https://en.wikipedia.org/wiki/Computer_chess   \n",
      "1  https://www.1stsource.com/advice/business/sbr_...   \n",
      "2        https://www.youtube.com/watch?v=tlThdr3O5Qo   \n",
      "3  https://aliz.ai/natural-language-processing-a-...   \n",
      "4  https://www.ibm.com/docs/SSZJPZ_9.1.0/com.ibm....   \n",
      "\n",
      "                             website with video link  \\\n",
      "0       https://en.wikipedia.org/wiki/Computer_chess   \n",
      "1  https://www.1stsource.com/advice/business/sbr_...   \n",
      "2        https://www.youtube.com/watch?v=tlThdr3O5Qo   \n",
      "3  https://aliz.ai/natural-language-processing-a-...   \n",
      "4  https://www.ibm.com/docs/SSZJPZ_9.1.0/com.ibm....   \n",
      "\n",
      "                            youtube video link  \\\n",
      "0  https://www.youtube.com/watch?v=h0QQJgiR_A8   \n",
      "1  https://www.youtube.com/watch?v=J5SXT9r2214   \n",
      "2  https://www.youtube.com/watch?v=__EoOvVkEMo   \n",
      "3  https://www.youtube.com/watch?v=5ctbvkAMQO4   \n",
      "4  https://www.youtube.com/watch?v=j0ZYnmPJUEM   \n",
      "\n",
      "                                           news link  \n",
      "0       https://en.wikipedia.org/wiki/Computer_chess  \n",
      "1  https://www.1stsource.com/advice/business/sbr_...  \n",
      "2        https://www.youtube.com/watch?v=tlThdr3O5Qo  \n",
      "3  https://aliz.ai/natural-language-processing-a-...  \n",
      "4  https://www.ibm.com/docs/SSZJPZ_9.1.0/com.ibm....  \n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(list(zip(Phrase,Website_link,website_video_link,youtube_link,news_link)),columns = ['Key phrase', 'Website Link','website with video link','youtube video link','news link'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002219c1",
   "metadata": {},
   "source": [
    "### Exporting links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5d2da511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter filename: WWW\n"
     ]
    }
   ],
   "source": [
    "filepath = input(\"Enter filename: \")\n",
    "filepath = filepath + '.xlsx'\n",
    "with open(filepath, 'w+') as fp:\n",
    "    pass\n",
    "df.to_excel(filepath,index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
