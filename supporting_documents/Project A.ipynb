{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3deff9fb",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3b7a5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2 \n",
    "import pytesseract as pt\n",
    "from PIL import Image\n",
    "pt.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract\"\n",
    "import nltk\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58220dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\ROSHINI R\n",
      "[nltk_data]     S\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\ROSHINI R\n",
      "[nltk_data]     S\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ROSHINI R S\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "017a5f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83459af6",
   "metadata": {},
   "source": [
    "### OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9443bde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OCR\n",
    "def img_txt(img_path):\n",
    "    global text\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img, None, fx = 1,fy = 1)\n",
    "    kernel = np.ones((1, 1), np.uint8)\n",
    "    img = cv2.dilate(img, kernel, iterations=1)\n",
    "    img = cv2.erode(img, kernel, iterations=1)\n",
    "    blurred=cv2.GaussianBlur(img,(5,5),0)\n",
    "    edged=cv2.Canny(blurred,30,50)\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    adpt = cv2.adaptiveThreshold(gray,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY, 85,10)\n",
    "    text = pt.image_to_string(adpt)\n",
    "    text = text.replace('\\n',' ')\n",
    "    print(text)\n",
    "    cv2.imshow(\"adaptive threshold\",adpt)\n",
    "    cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c083f1",
   "metadata": {},
   "source": [
    "### Geting Input from the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19d5f951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escape hit, closing...\n"
     ]
    }
   ],
   "source": [
    "#inupt image through webcam\n",
    "cam = cv2.VideoCapture(0)\n",
    "\n",
    "cv2.namedWindow(\"Capture\")\n",
    "\n",
    "img_counter = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "    if not ret:\n",
    "        print(\"failed to grab frame\")\n",
    "        break\n",
    "    \n",
    "    cv2.imshow(\"Capture\", frame)\n",
    " \n",
    "    k = cv2.waitKey(1)\n",
    "    if k%256 == 27:\n",
    "        # ESC pressed\n",
    "        print(\"Escape hit, closing...\")\n",
    "        break\n",
    "    elif k%256 == 32:\n",
    "        # SPACE pressed  \n",
    "        img_name = \"opencv_frame_{}.png\".format(img_counter)\n",
    "        cv2.imwrite(img_name, frame)\n",
    "        print(\"{} written!\".format(img_name))\n",
    "        img_counter += 1\n",
    "    \n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0659d805",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the image from local file\n",
    "img_name = 'sample_image.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d905ba07",
   "metadata": {},
   "source": [
    "### Display raw text from the Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb4ba1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanned text:\n",
      "\n",
      "Arti  â€˜ial Intelligence And Machine Learning  Artificial Intelligence (Al) has gained widespread popularity in recent years, with its application flooding every business sector. Al has gained astonishingly great acceptance in the world of portable technology, by making various functions available at your fingertips. The fast speed of Al development, and accomplishments in automation, automated vehicles, the capacity to beat people at mind games, and computerized user support mean that Al is a progressive technology that will receive extraordinary rewards over the long haul.  The ongoing Al environment comprises robotics, machine learning (ML), and artificial neural networks (ANNs) that are improving the semantic, numerical, and legitimate thinking skills of any framework using Al. The two main variables driving the quick reception of Al are top caliber, versatile learning models, and the need to process a lot of information at reasonable costs. Very much like every other industry, Al is also affecting the application development world. With this in mind, how do we understand what Al and ML mean for iOS application development in 2023? \n"
     ]
    }
   ],
   "source": [
    "#Process the image and print text\n",
    "print(\"Scanned text:\\n\")\n",
    "img_txt(img_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e98c6d",
   "metadata": {},
   "source": [
    "### Summarize the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f939463f",
   "metadata": {},
   "source": [
    "### Using Spacy to summarise the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4074ee34",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E941] Can't find model 'en'. It looks like you're trying to load a model from a shortcut, which is obsolete as of spaCy v3.0. To load the model, use its full name instead:\n\nnlp = spacy.load(\"en_core_web_sm\")\n\nFor more details on the available models, see the models directory: https://spacy.io/models. If you want to create a blank model, use spacy.blank: nlp = spacy.blank(\"en\")",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(text)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# len(list(doc.sents))\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     31\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     37\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\util.py:426\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[0;32m    424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m load_model_from_path(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[1;32m--> 426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E941] Can't find model 'en'. It looks like you're trying to load a model from a shortcut, which is obsolete as of spaCy v3.0. To load the model, use its full name instead:\n\nnlp = spacy.load(\"en_core_web_sm\")\n\nFor more details on the available models, see the models directory: https://spacy.io/models. If you want to create a blank model, use spacy.blank: nlp = spacy.blank(\"en\")"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en\")\n",
    "doc = nlp(text)\n",
    "# len(list(doc.sents))\n",
    "\n",
    "keyword = []\n",
    "stopwords = list(STOP_WORDS)\n",
    "pos_tag = ['PROPN', 'ADJ', 'NOUN', 'VERB']\n",
    "for token in doc:\n",
    "    if(token.text in stopwords or token.text in punctuation):\n",
    "        continue\n",
    "    if(token.pos_ in pos_tag):\n",
    "        keyword.append(token.text)\n",
    "        \n",
    "freq_word = Counter(keyword)\n",
    "print(freq_word.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7adb59d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keyword' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m max_freq \u001b[38;5;241m=\u001b[39m Counter(\u001b[43mkeyword\u001b[49m)\u001b[38;5;241m.\u001b[39mmost_common(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m freq_word\u001b[38;5;241m.\u001b[39mkeys():  \n\u001b[0;32m      3\u001b[0m         freq_word[word] \u001b[38;5;241m=\u001b[39m (freq_word[word]\u001b[38;5;241m/\u001b[39mmax_freq)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'keyword' is not defined"
     ]
    }
   ],
   "source": [
    "max_freq = Counter(keyword).most_common(1)[0][1]\n",
    "for word in freq_word.keys():  \n",
    "        freq_word[word] = (freq_word[word]/max_freq)\n",
    "freq_word.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6162dd42",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m sent_strength\u001b[38;5;241m=\u001b[39m{}\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdoc\u001b[49m\u001b[38;5;241m.\u001b[39msents:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m sent:\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;129;01min\u001b[39;00m freq_word\u001b[38;5;241m.\u001b[39mkeys():\n",
      "\u001b[1;31mNameError\u001b[0m: name 'doc' is not defined"
     ]
    }
   ],
   "source": [
    "sent_strength={}\n",
    "for sent in doc.sents:\n",
    "    for word in sent:\n",
    "        if word.text in freq_word.keys():\n",
    "            if sent in sent_strength.keys():\n",
    "                sent_strength[sent]+=freq_word[word.text]\n",
    "            else:\n",
    "                sent_strength[sent]=freq_word[word.text]\n",
    "# print(sent_strength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b95d602",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarized_sentences = nlargest(4, sent_strength, key=sent_strength.get)\n",
    "# print(summarized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c3ca6f2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized text: \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_sentences = [ w.text for w in summarized_sentences ]\n",
    "summary = ' '.join(final_sentences)\n",
    "print(\"Summarized text: \\n\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0395fac",
   "metadata": {},
   "source": [
    "### Extracting key phrases from the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08273d47",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'RAKE'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mRAKE\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01moperator\u001b[39;00m\n\u001b[0;32m      3\u001b[0m stop_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mROSHINI R S\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124manaconda3\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mLib\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msite-packages\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mRAKE\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mstoplists\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSmartStopList.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'RAKE'"
     ]
    }
   ],
   "source": [
    "import RAKE\n",
    "import operator\n",
    "stop_dir = r\"C:\\Users\\ROSHINI R S\\anaconda3\\Lib\\site-packages\\RAKE\\stoplists\\SmartStopList.txt\"\n",
    "rake_object=RAKE.Rake(stop_dir)\n",
    "def Sort_Tuple(tup):\n",
    "    tup.sort(key=lambda x: x[1])\n",
    "    return tup\n",
    "keywords = Sort_Tuple(rake_object.run(summary))[-5:]\n",
    "print(\"Keywords: \\n\",keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "376fb671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyphrases: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['chess-playing computers',\n",
       " 'accomplish specific tasks',\n",
       " 'self-driving cars',\n",
       " 'natural language processing',\n",
       " 'processing large amounts']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Phrase=[]\n",
    "for i in keywords:\n",
    "    Phrase.append(i[0])\n",
    "print(\"Keyphrases: \")\n",
    "Phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffeb50d6",
   "metadata": {},
   "source": [
    "### Finding relevant links from the keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b78bd553",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Automate google search for getting relevant links\n",
    "import requests\n",
    "import urllib.request\n",
    "import re\n",
    "\n",
    "try:\n",
    "    from googlesearch import search\n",
    "except ImportError:\n",
    "    print(\"No module named 'google' found\")\n",
    "Website_link=[]\n",
    "website_video_link=[]\n",
    "youtube_link=[]\n",
    "news_link=[]\n",
    "\n",
    "for i in Phrase:\n",
    "    for j in search(i, tld=\"co.in\", num=1, start=0,stop=1,pause=2,safe='on'):\n",
    "        Website_link.append(j)\n",
    "    for k in search(i, tld=\"co.in\", num=1, start=0,stop=1,pause=2,tpe='vid',safe='on'):\n",
    "        website_video_link.append(k)\n",
    "    for l in search(i, tld=\"co.in\", num=1, start=0,stop=1,pause=2,tpe='nws',safe='on'):\n",
    "        news_link.append(l)\n",
    "\n",
    "for m in Phrase:\n",
    "  query=m.replace(\" \",\"+\")\n",
    "  html=urllib.request.urlopen(\"https://www.youtube.com/results?search_query=\"+query)\n",
    "  video_ids = re.findall(r\"watch\\?v=(\\S{11})\", html.read().decode())\n",
    "  link=(\"https://www.youtube.com/watch?v=\" + video_ids[0])\n",
    "  youtube_link.append(link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cc7d1bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Keywords  \\\n",
      "0  existing speech conversion apps allows live co...   \n",
      "1  existing speech conversion applications conver...   \n",
      "2                      convert old speech recordings   \n",
      "\n",
      "                                              Link 1  \\\n",
      "0  https://www.researchgate.net/publication/25203...   \n",
      "1        https://www.youtube.com/watch?v=6lLSx0nUxaE   \n",
      "2      https://en.wikipedia.org/wiki/Registered_mail   \n",
      "\n",
      "                                              Link 2  \\\n",
      "0  https://www.techradar.com/in/news/best-speech-...   \n",
      "1  https://www.researchgate.net/publication/31248...   \n",
      "2  https://www.rev.com/blog/resources/how-to-conv...   \n",
      "\n",
      "                                              Link 3  \n",
      "0  https://www.folio3.ai/blog/best-free-speech-to...  \n",
      "1  https://www.ijert.org/research/implementation-...  \n",
      "2  https://www.komando.com/downloads/convert-audi...  \n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(list(zip(Phrase,Website_link,website_video_link,youtube_link,news_link)),columns = ['Key phrase', 'Website Link','website with video link','youtube video link','news link'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002219c1",
   "metadata": {},
   "source": [
    "### Exporting links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5d2da511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter filename: ReferenceLink_SpeechCon\n"
     ]
    }
   ],
   "source": [
    "filepath = input(\"Enter filename: \")\n",
    "filepath = filepath + '.xlsx'\n",
    "with open(filepath, 'w+') as fp:\n",
    "    pass\n",
    "df.to_excel(filepath,index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
