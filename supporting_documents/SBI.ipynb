{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3deff9fb",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3b7a5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2 \n",
    "import pytesseract as pt\n",
    "from PIL import Image\n",
    "pt.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract\"\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58220dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\ROSHINI R\n",
      "[nltk_data]     S\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\ROSHINI R\n",
      "[nltk_data]     S\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ROSHINI R S\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm',disable=['ner','textcat'])\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "017a5f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83459af6",
   "metadata": {},
   "source": [
    "### OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9443bde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OCR\n",
    "def img_txt(img_path):\n",
    "    global text\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img, None, fx = 1,fy = 1)\n",
    "    kernel = np.ones((1, 1), np.uint8)\n",
    "    img = cv2.dilate(img, kernel, iterations=1)\n",
    "    img = cv2.erode(img, kernel, iterations=1)\n",
    "    blurred=cv2.GaussianBlur(img,(5,5),0)\n",
    "    edged=cv2.Canny(blurred,30,50)\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    adpt = cv2.adaptiveThreshold(gray,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY, 85,10)\n",
    "    text = pt.image_to_string(adpt)\n",
    "    text = text.replace('\\n',' ')\n",
    "    print(text)\n",
    "    cv2.imshow(\"adaptive threshold\",adpt)\n",
    "    cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c083f1",
   "metadata": {},
   "source": [
    "### Getting Input from the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5f79e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed to grab frame\n"
     ]
    }
   ],
   "source": [
    "# #inupt image through webcam\n",
    "# cam = cv2.VideoCapture(0)\n",
    "\n",
    "# cv2.namedWindow(\"Capture\")\n",
    "\n",
    "# img_counter = 0\n",
    "\n",
    "# while True:\n",
    "#     ret, frame = cam.read()\n",
    "#     if not ret:\n",
    "#         print(\"failed to grab frame\")\n",
    "#         break\n",
    "    \n",
    "#     cv2.imshow(\"Capture\", frame)\n",
    " \n",
    "#     k = cv2.waitKey(1)\n",
    "#     if k%256 == 27:\n",
    "#         # ESC pressed\n",
    "#         print(\"Escape hit, closing...\")\n",
    "#         break\n",
    "#     elif k%256 == 32:\n",
    "#         # SPACE pressed  \n",
    "#         img_name = \"opencv_frame_{}.png\".format(img_counter)\n",
    "#         cv2.imwrite(img_name, frame)\n",
    "#         print(\"{} written!\".format(img_name))\n",
    "#         img_counter += 1\n",
    "    \n",
    "# cam.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0659d805",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the image from local file\n",
    "img_name = r'C:\\Users\\ROSHINI R S\\OneDrive\\Desktop\\image1.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d905ba07",
   "metadata": {},
   "source": [
    "### Display raw text from the Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4ba1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanned text:\n",
      "\n",
      "   Release  Amsterdam October [25], 2010  STRICTLY EMBARGOED UNTIL OCTOBER 25, 2010  Abu Dhabi Commercial Bank selects Deutsche Bank for cross-currency payments  Deutsche Bank and Abu Dhabi Commercial Bank (ADCB) today announced an allianca in cross-currency payment solutions to facilitate payments in line with their growth in the United Arab Emirates (UAE). This expands ADCB's existing payments and cash management relationship with Deutsche Bank who has been serving a8 ADCB's main euro clearing bank for many years.  ADGB, one of the largest banks in the UAE, is a diversified full service bank, which ia active in all banking services that span corporate, retail and commercial banking. Additionally, covering the areas of treasury derivatives, infrastructure finance, private banking and wealth management, ADCB has a network of 49 branches and 340,000 retail customers but with its recent acquisition of the retail banking business of the Royal Bank of Scotland in the UAE, ADCB’s franchise will grow by hundreds of thousands of customers.     ADCB and its clients are using Deutsche Bank's FX4Cash platform for global cross-currency payments. FX4Cash was developed by Deutsche Bank's Global Transaction Banking in collaboration with Global Markets to leverage the Bank’s number one FX role as well as its leading clearing positions in auro and US dollar.  Utilizing FX4Cash, cllents are able to send and receive cross-currency payments in over 120 different currencies. FX4Cash provides Deutsche Bank clients with new functionality, added control and revenue opportunities for servicing their cross-currency payment requirements.  Padmanabhan Mishra, Head of Financial Institutions at ADCB, sald, “We chose to expand our relationship with Deutsche Bank to support efficient execution of a wider range of cross-currency payments for our growing client base. We continue to be impressed with Deutsche Bank's product innovation, delivery and customer service. We view this alliance as another milestone for continued future cooperation between our institutions,”  ‘Wolfgang Wagner /Paul Camp, tide, Deutsche Bank, said “We are pleased to  Release 1/3 \f",
      "\n"
     ]
    }
   ],
   "source": [
    "#Process the image and print text\n",
    "print(\"Scanned text:\\n\")\n",
    "img_txt(img_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e98c6d",
   "metadata": {},
   "source": [
    "### Summarize the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f939463f",
   "metadata": {},
   "source": [
    "### Using Spacy to summarise the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d68190bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('lion', 7), ('mouse', 5), ('jungle', 2), ('started', 2), ('set', 2)]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en')\n",
    "doc = nlp(text)\n",
    "type(doc)\n",
    "len(list(doc.sents))\n",
    "keyword = []\n",
    "stopwords = list(STOP_WORDS)\n",
    "pos_tag = ['PROPN', 'ADJ', 'NOUN', 'VERB']\n",
    "for token in doc:\n",
    "    if(token.text in stopwords or token.text in punctuation):\n",
    "        continue\n",
    "    if(token.pos_ in pos_tag):\n",
    "        keyword.append(token.text)\n",
    "        \n",
    "freq_word = Counter(keyword)\n",
    "print(freq_word.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7adb59d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lion', 1.0),\n",
       " ('mouse', 0.7142857142857143),\n",
       " ('jungle', 0.2857142857142857),\n",
       " ('started', 0.2857142857142857),\n",
       " ('set', 0.2857142857142857)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_freq = Counter(keyword).most_common(1)[0][1]\n",
    "for word in freq_word.keys():  \n",
    "        freq_word[word] = (freq_word[word]/max_freq)\n",
    "freq_word.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6162dd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_strength={}\n",
    "for sent in doc.sents:\n",
    "    for word in sent:\n",
    "        if word.text in freq_word.keys():\n",
    "            if sent in sent_strength.keys():\n",
    "                sent_strength[sent]+=freq_word[word.text]\n",
    "            else:\n",
    "                sent_strength[sent]=freq_word[word.text]\n",
    "# print(sent_strength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b95d602",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarized_sentences = nlargest(4, sent_strength, key=sent_strength.get)\n",
    "# print(summarized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c3ca6f2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized text: \n",
      "\n",
      "He was about to eat the mouse when the mouse desperately requested the lion to set him free. Soon, the mouse walked past and noticed the lion in trouble. The lion laughed at the mouse's confidence and let him go.   Quickly, he ran and gnawed on the ropes to set the lion free.\n"
     ]
    }
   ],
   "source": [
    "final_sentences = [ w.text for w in summarized_sentences ]\n",
    "summary = ' '.join(final_sentences)\n",
    "print(\"Summarized text: \\n\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0395fac",
   "metadata": {},
   "source": [
    "### Extracting key phrases from the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08273d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords: \n",
      " [('mouse', 2.0), ('lion free', 3.0), ('lion laughed', 3.5), ('mouse desperately requested', 8.0), ('mouse walked past', 8.0)]\n"
     ]
    }
   ],
   "source": [
    "import RAKE\n",
    "import operator\n",
    "stop_dir = r\"C:\\Users\\ROSHINI R S\\anaconda3\\Lib\\site-packages\\RAKE\\stoplists\\SmartStopList.txt\"\n",
    "rake_object=RAKE.Rake(stop_dir)\n",
    "def Sort_Tuple(tup):\n",
    "    tup.sort(key=lambda x: x[1])\n",
    "    return tup\n",
    "keywords = Sort_Tuple(rake_object.run(summary))[-5:]\n",
    "print(\"Keywords: \\n\",keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "376fb671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyphrases: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['mouse',\n",
       " 'lion free',\n",
       " 'lion laughed',\n",
       " 'mouse desperately requested',\n",
       " 'mouse walked past']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Phrase=[]\n",
    "for i in keywords:\n",
    "    Phrase.append(i[0])\n",
    "print(\"Keyphrases: \")\n",
    "Phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffeb50d6",
   "metadata": {},
   "source": [
    "### Finding relevant links from the keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311ebc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Automate google search for getting relevant links\n",
    "import requests\n",
    "import urllib.request\n",
    "import re\n",
    "\n",
    "try:\n",
    "    from googlesearch import search\n",
    "except ImportError:\n",
    "    print(\"No module named 'google' found\")\n",
    "Website_link=[]\n",
    "website_video_link=[]\n",
    "youtube_link=[]\n",
    "news_link=[]\n",
    "\n",
    "for i in Phrase:\n",
    "    for j in search(i, tld=\"co.in\", num=1, start=0,stop=1,pause=2,safe='on'):\n",
    "        Website_link.append(j)\n",
    "    for k in search(i, tld=\"co.in\", num=1, start=0,stop=1,pause=2,tpe=\"vid\",safe='on'):\n",
    "        website_video_link.append(k)\n",
    "    for l in search(i, tld=\"co.in\", num=1, start=0,stop=1,pause=2,tpe=\"nws\",safe='on'):\n",
    "        news_link.append(l)\n",
    "\n",
    "for m in Phrase:\n",
    "  query=m.replace(\" \",\"+\")\n",
    "  html=urllib.request.urlopen(\"https://www.youtube.com/results?search_query=\"+query)\n",
    "  video_ids = re.findall(r\"watch\\?v=(\\S{11})\", html.read().decode())\n",
    "  link=(\"https://www.youtube.com/watch?v=\" + video_ids[0])\n",
    "  youtube_link.append(link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc7d1bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Key phrase  \\\n",
      "0      chess-playing computers   \n",
      "1    accomplish specific tasks   \n",
      "2            self-driving cars   \n",
      "3  natural language processing   \n",
      "4     processing large amounts   \n",
      "\n",
      "                                        Website Link  \\\n",
      "0       https://en.wikipedia.org/wiki/Computer_chess   \n",
      "1  https://www.1stsource.com/advice/business/sbr_...   \n",
      "2        https://www.youtube.com/watch?v=tlThdr3O5Qo   \n",
      "3  https://aliz.ai/natural-language-processing-a-...   \n",
      "4  https://www.ibm.com/docs/SSZJPZ_9.1.0/com.ibm....   \n",
      "\n",
      "                             website with video link  \\\n",
      "0       https://en.wikipedia.org/wiki/Computer_chess   \n",
      "1  https://www.1stsource.com/advice/business/sbr_...   \n",
      "2        https://www.youtube.com/watch?v=tlThdr3O5Qo   \n",
      "3  https://aliz.ai/natural-language-processing-a-...   \n",
      "4  https://www.ibm.com/docs/SSZJPZ_9.1.0/com.ibm....   \n",
      "\n",
      "                            youtube video link  \\\n",
      "0  https://www.youtube.com/watch?v=h0QQJgiR_A8   \n",
      "1  https://www.youtube.com/watch?v=J5SXT9r2214   \n",
      "2  https://www.youtube.com/watch?v=__EoOvVkEMo   \n",
      "3  https://www.youtube.com/watch?v=5ctbvkAMQO4   \n",
      "4  https://www.youtube.com/watch?v=j0ZYnmPJUEM   \n",
      "\n",
      "                                           news link  \n",
      "0       https://en.wikipedia.org/wiki/Computer_chess  \n",
      "1  https://www.1stsource.com/advice/business/sbr_...  \n",
      "2        https://www.youtube.com/watch?v=tlThdr3O5Qo  \n",
      "3  https://aliz.ai/natural-language-processing-a-...  \n",
      "4  https://www.ibm.com/docs/SSZJPZ_9.1.0/com.ibm....  \n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(list(zip(Phrase,Website_link,website_video_link,youtube_link,news_link)),columns = ['Key phrase', 'Website Link','website with video link','youtube video link','news link'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002219c1",
   "metadata": {},
   "source": [
    "### Exporting links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5d2da511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter filename: WWW\n"
     ]
    }
   ],
   "source": [
    "filepath = input(\"Enter filename: \")\n",
    "filepath = filepath + '.xlsx'\n",
    "with open(filepath, 'w+') as fp:\n",
    "    pass\n",
    "df.to_excel(filepath,index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
